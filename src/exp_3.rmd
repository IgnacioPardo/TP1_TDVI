---
title: "Experimento 3"
author: 
    - "Zoe Borrone"
    - "Luca Mazzarello"
    - "Ignacio Pardo"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r echo=FALSE, message=FALSE, warning=FALSE}

# source("exp_3.R")

set.seed(364630336)

# Leer tabla en "outputs/tables/exp3.txt"
library(ggplot2) # For creating plots
library(dplyr) # For data manipulation
library(knitr) # For kable

filename_exp_results <- "outputs/tables/exp3.txt"

exp_results <- read.table(filename_exp_results, header=TRUE, sep="\t")

# Replace 0 with FALSE and 1 with TRUE
exp_results$DISCRETIZED <- ifelse(exp_results$DISCRETIZED == 0, FALSE, TRUE)

churn <- exp_results %>%
  filter(dataset_name == "Churn")

heart <- exp_results %>%
  filter(dataset_name == "Heart")

student <- exp_results %>%
  filter(dataset_name == "Student")

churn_disc <- churn %>%
  group_by(maxdepth, DISCRETIZED) %>%
  summarize(mean_auc = mean(auc), .groups = "drop")

heart_disc <- heart %>%
    group_by(maxdepth, DISCRETIZED) %>%
    summarize(mean_auc = mean(auc), .groups = "drop")

student_disc <- student %>%
    group_by(maxdepth, DISCRETIZED) %>%
    summarize(mean_auc = mean(auc), .groups = "drop")

```

## Introducción

Para desarrollar este experimento se planteo testear el comportamiento del modelo al aplicar **One-Hot Encoding** (OHE) a las variables de los datasets. El objetivo es observar si el modelo mejora su performance al aplicarle OHE a las variables categóricas de los datasets. Además veremos si el modelo se ve afectado por la imputación de los valores faltantes.

## Resultados

A simple vista parecería ser que los modelos entrenados con los datasets _Churn_ y _Student_ no se ven demasiado alterados al hacerles **OHE**, pero al modelo entrenado con el dataset de _Heart Disease_ parece alcanzar mayor performance, especialmente al imputarle los valores faltantes, pero además luego de caer a un menor valor mínimo de AUC parece remontar un poco más su performance que aquel modelo al que no se lo preprocesó haciendo OHE.

Como en la figura no se logran observar diferencias muy significativas entre los resultados obtenidos con y sin *One-Hot Encoding* para los modelos de _Churn_ y _Student_ generamos 3 figuras nuevas una por cada dataset para poder observar mejor los resultados.


![Resultados sin (0/izquierda) y con (1/derecha) One-Hot Encoding](outputs/plots/exp3.jpg)


\newpage

### Churn

Como se puede observar en la "_Tabla 1_", para el dataset de *Iranian Churn* el mayor valor de AUC alcanzado para el dataset sin *One-Hot Encoding* se obtiene con un árbol de `maxdepth=5` cuando no se imputan los valores faltantes y con un árbol de `maxdepth=7` cuando si se imputan los valores faltantes con la media. 

En cambio, para el dataset con *One-Hot Encoding* se obtiene el mayor valor de AUC con un árbol de `maxdepth=8` cuando no se imputan los valores faltantes y con un árbol de `maxdepth=7` cuando se imputan los valores faltantes con la media. Además parecería ser que luego de llegar a sus respectivos máximos para cada caso, los modelos con *One-Hot Encoding* se mantienen más cercanos al máximo inicial que aquellos sin *One-Hot Encoding*. Esto podría ser porque su pico no fue dado por volverse overfitteado. 

```{r echo=FALSE, message=FALSE, warning=FALSE}
# Get tree depth with max AUC for each imputation method

churn_maxs <- churn_disc %>%
  group_by(DISCRETIZED) %>%
  filter(mean_auc == max(mean_auc)) %>%
  select(maxdepth, mean_auc, DISCRETIZED)

churn_maxs <- churn_maxs %>%
    arrange(DISCRETIZED)

# Replace TRUE/FALSE w Yes/No in ohe column
churn_maxs$DISCRETIZED <- ifelse(churn_maxs$DISCRETIZED == TRUE, "Yes", "No")

kable(churn_maxs, caption = "Churn Max AUCs")
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
ggplot(churn_disc, aes(x = maxdepth, y = mean_auc)) +
  geom_line() +
  geom_point() +
  facet_wrap(~DISCRETIZED) +
  labs(title = "Churn",
       x = "Max Depth",
       y = "Mean AUC",
       color = "Imputed") +
  theme_minimal()
```

\newpage

### Heart

```{r echo=FALSE, message=FALSE, warning=FALSE}
# Get tree depth with max AUC for each imputation method

heart_maxs <- heart_disc %>%
  group_by(DISCRETIZED) %>%
  filter(mean_auc == max(mean_auc)) %>%
  select(maxdepth, mean_auc, DISCRETIZED)

# Sort by ohe

heart_maxs <- heart_maxs %>%
  arrange(DISCRETIZED)

```

Como planteamos anteriormente, al aplicar **OHE** al dataset de _Heart Disease_ previo a entrenar los modelos, podemos observar en la Figura Heart que dichos modelos "exageran" sus métricas de AUC respecto a los modelos entrenados con el dataset sin *One-Hot Encoding*. Esto se debe a que al aplicar **OHE** a las variables categóricas, estas pasan a ser numéricas y por lo tanto el modelo puede entrenarse con ellas. En contraste con el dataset de _Churn_ que contiene 3/13 variables categóricas, el dataset _Heart Disease_ contiene 5/11 variables categóricas lo que parece mejorar la performance.

Para los modelos entrenados con el dataset de _Heart Disease_ podemos observar en la Tabla 2 como para tanto imputar datos faltantes y como para hacer o no **OHE**, los arboles alcanzan todos sus máximos valores de AUC cuando su `maxdepth=4`. Sin embargo, el valor medio de sus AUC entre los repetidos entrenamientos de cada modelo con dichos hiper-parametros es mayor para los modelos con *One-Hot Encoding* (~0.894 sin imputar y ~0.881 imputando) que para los modelos sin *One-Hot Encoding* (~0.871 sin imputar y ~0.873 imputando). 

Aunque a simple vista parecía ser que el modelo alcanza menores mínimos al hacer **OHE**, en la Tabla 3 podemos observar que la diferencia es minúscula.

```{r echo=FALSE, message=FALSE, warning=FALSE}

# Replace TRUE/FALSE w Yes/No in ohe column
heart_maxs$DISCRETIZED <- ifelse(heart_maxs$DISCRETIZED == TRUE, "Yes", "No")

kable(heart_maxs, caption = "Heart Max AUCs")

# Min values

heart_mins <- heart_disc %>%
  group_by(DISCRETIZED) %>%
  filter(mean_auc == min(mean_auc)) %>%
  select(maxdepth, mean_auc, DISCRETIZED)

# Sort by ohe

heart_mins <- heart_mins %>%
  arrange(DISCRETIZED)

# Replace TRUE/FALSE w Yes/No in ohe column
heart_mins$DISCRETIZED <- ifelse(heart_mins$DISCRETIZED == TRUE, "Yes", "No")

kable(heart_mins, caption = "Heart Min AUCs")
```
De todas formas, las "colas" para los modelos de mayor profundidad, parecen tener mayores valores medios de AUC que para los modelos sin *One-Hot Encoding*, casi como si el modelo fuese a repuntar. Esto podría ser porque el modelo no se volvió overfitteado y por lo tanto puede seguir aprendiendo de los datos, aunque al ser mínimo no nos pareció suficiente como para avanzar.

\newpage

```{r echo=FALSE, message=FALSE, warning=FALSE}
ggplot(heart_disc, aes(x = maxdepth, y = mean_auc)) +
  geom_line() +
  geom_point() +
  facet_wrap(~DISCRETIZED) +
  labs(title = "Heart",
       x = "Max Depth",
       y = "Mean AUC",
       color = "Imputed") +
  theme_minimal()

```

\newpage

### Student

El último caso no respeta lo planteado para los datasets anteriores, ya que al aplicarle **OHE** al dataset de Student Performance previo a entrenar los Arboles de Decisión, podemos observar que dichos modelos no alcanzan los mismos valores medios máximos de AUC que aquellos a los que no se le aplico **OHE**. Esto creemos que se debe a que el dataset aunque no contenía muchas variables numéricas (13/30) con respecto al total, el resto de las variables categóricas se veían distribuidas entre binarias (13/30) y nominales (4/30), lo cual parecería ser empeora la performance del modelo al aplicarle **OHE**. Probablemente ocurra por convertir las variables binarias más que por las nominales.

```{r echo=FALSE, message=FALSE, warning=FALSE}
ggplot(student_disc, aes(x = maxdepth, y = mean_auc)) +
  geom_line() +
  geom_point() +
  facet_wrap(~DISCRETIZED) +
  labs(title = "Student",
       x = "Max Depth",
       y = "Mean AUC",
       color = "Imputed") +
  theme_minimal()
```

## Conclusiones    

Aunque no parecería haber un patrón discernible entre los comportamientos de los modelos con y sin **OHE** entre los distintos datasets, una buena interpretación es que sus curvas de performance se ven "escaladas", ya sea para mejor como en "Heart Disease" como para peor en "Iranian Churn" y "Student Performance". Nuestra interpretación esta basada en que este "escalado" esta dado por la proporción de variables categóricas contra las variables numéricas de cada dataset. En aquel dataset cuya proporción era necesariamente mayor (Heart 5/11), el modelo se veía beneficiado por el **OHE** y en aquellos cuya proporción era menor (Churn 3/13), el modelo se veía perjudicado, aunque mínimo, por el **OHE**.
